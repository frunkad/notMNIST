{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First import all the necessary files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/frunkad/anaconda3/lib/python3.6/site-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n"
     ]
    }
   ],
   "source": [
    "import numpy as np \n",
    "from scipy.io import loadmat\n",
    "import sklearn\n",
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "from string import ascii_lowercase\n",
    "from random import randint\n",
    "import sys,os\n",
    "\n",
    "tf.logging.set_verbosity(tf.logging.INFO)\n",
    "os.environ[\"TF_CPP_MIN_LOG_LEVEL\"]=\"2\"\n",
    "class bcolors:\n",
    "    HEADER = '\\033[95m'\n",
    "    OKBLUE = '\\033[94m'\n",
    "    OKGREEN = '\\033[92m'\n",
    "    WARNING = '\\033[93m'\n",
    "    FAIL = '\\033[91m'\n",
    "    ENDC = '\\033[0m'\n",
    "    BOLD = '\\033[1m'\n",
    "    UNDERLINE = '\\033[4m'\n",
    "\n",
    "DATASET = \"small\" # or \"large\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "mat_file = loadmat(\"datasets/notMNIST_small.mat\")\n",
    "L = len(mat_file['labels'])\n",
    "\n",
    "mat_file['images'] = np.array([mat_file['images'][:,:,i] for i in range(L)])\n",
    "# vf = np.vectorize(lambda x: ascii_lowercase[int(x)])\n",
    "vf = np.vectorize(lambda x: int(x))\n",
    "mat_file['labels'] = vf(mat_file['labels'])\n",
    "\n",
    "seed = randint(1,1000)\n",
    "np.random.seed(seed)\n",
    "np.random.shuffle(mat_file['images'])\n",
    "np.random.seed(seed)\n",
    "np.random.shuffle(mat_file['labels'])\n",
    "\n",
    "test_len = L//4\n",
    "train_len = L - test_len\n",
    "trainX = mat_file['images'][test_len:]\n",
    "trainy = mat_file['labels'][test_len:]\n",
    "testX = mat_file['images'][:test_len]\n",
    "testy = mat_file['labels'][:test_len]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Images: 14043\n",
      "Testing Images: 4681\n"
     ]
    }
   ],
   "source": [
    "print(\"Training Images: {}\\nTesting Images: {}\".format(train_len,test_len))\n",
    "trainX /= 255\n",
    "testX /= 255\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def tostr(s):\n",
    "    if callable(s):\n",
    "        return s.__name__\n",
    "    else:\n",
    "        return str(s)\n",
    "\n",
    "def create_networks():\n",
    "    global trainX,testX,testy,trainy\n",
    "#     trainX = np.reshape(trainX,(train_len,28*28))\n",
    "#     testX = np.reshape(testX,(test_len,28*28))\n",
    "    trainy = tf.keras.utils.to_categorical(trainy,10)\n",
    "    testy = tf.keras.utils.to_categorical(testy,10)\n",
    "\n",
    "\n",
    "    model = tf.keras.Sequential()\n",
    "    model.add(tf.keras.layers.Dense(64, activation=tf.nn.relu, input_shape=(784,)))\n",
    "    model.add(tf.keras.layers.Dropout(0.65))\n",
    "    model.add(tf.keras.layers.Dense(10,activation=tf.nn.softmax))\n",
    "    # \n",
    "    # We will now compile and print out a summary of our model\n",
    "    model.compile(loss='categorical_crossentropy',\n",
    "                optimizer='rmsprop',\n",
    "                metrics=['accuracy'])\n",
    "    return model\n",
    "\n",
    "def try_model(\n",
    "                epochs = 5,\n",
    "                save = False,\n",
    "                batch_size = None,\n",
    "                model = create_networks()\n",
    "            ):\n",
    "    try_model.count += 1\n",
    "\n",
    "\n",
    "    \n",
    "    model.fit(trainX,trainy,epochs=epochs,verbose=1,batch_size=batch_size)\n",
    "    lossed,accuracy = model.evaluate(testX,testy,verbose=1)\n",
    "    print(\"====================================\\nModel \",try_model.count,\" : \")\n",
    "    # print(\"\\tActivator 1: {}\\n\\tActivator 2: {}\\n\\tLoss: {}\\n\\tOptimizer: {}\\n\\tEpochs: {}\".format(tostr(activator1),tostr(activator2),loss,tostr(optimizer),epochs))\n",
    "    model.summary()\n",
    "    print(bcolors.OKGREEN+\"\\tAccuracy: {}\".format(accuracy)+bcolors.ENDC)\n",
    "    print(bcolors.WARNING+\"\\tLoss: {}\".format(lossed)+bcolors.ENDC)\n",
    "    if save:\n",
    "        tf.keras.models.save_model(model,\"model-\"+DATASET+\"-\"+time.ctime()+\".hdf5\")\n",
    "\n",
    "try_model.count = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# try_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainX = np.expand_dims(trainX,-1)\n",
    "testX = np.expand_dims(trainX,-1)\n",
    "\n",
    "\n",
    "trainy = tf.keras.utils.to_categorical(trainy,10)\n",
    "testy = tf.keras.utils.to_categorical(testy,10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "img_input = tf.keras.layers.Input(shape=(28, 28, 1))\n",
    "\n",
    "# First convolution extracts 16 filters that are 3x3\n",
    "# Convolution is followed by max-pooling layer with a 2x2 window\n",
    "x = tf.keras.layers.Conv2D(16, 3, activation='relu')(img_input)\n",
    "x = tf.keras.layers.MaxPooling2D(2)(x)\n",
    "\n",
    "# Second convolution extracts 32 filters that are 3x3\n",
    "# Convolution is followed by max-pooling layer with a 2x2 window\n",
    "x = tf.keras.layers.Conv2D(32, 3, activation='relu')(x)\n",
    "x = tf.keras.layers.MaxPooling2D(2)(x)\n",
    "\n",
    "# Third convolution extracts 64 filters that are 3x3\n",
    "# Convolution is followed by max-pooling layer with a 2x2 window\n",
    "x = tf.keras.layers.Conv2D(64, 3, activation='relu')(x)\n",
    "x = tf.keras.layers.MaxPooling2D(2)(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_1 (InputLayer)         (None, 28, 28, 1)         0         \n",
      "_________________________________________________________________\n",
      "conv2d_1 (Conv2D)            (None, 26, 26, 16)        160       \n",
      "_________________________________________________________________\n",
      "max_pooling2d_1 (MaxPooling2 (None, 13, 13, 16)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_2 (Conv2D)            (None, 11, 11, 32)        4640      \n",
      "_________________________________________________________________\n",
      "max_pooling2d_2 (MaxPooling2 (None, 5, 5, 32)          0         \n",
      "_________________________________________________________________\n",
      "conv2d_3 (Conv2D)            (None, 3, 3, 64)          18496     \n",
      "_________________________________________________________________\n",
      "max_pooling2d_3 (MaxPooling2 (None, 1, 1, 64)          0         \n",
      "_________________________________________________________________\n",
      "flatten_1 (Flatten)          (None, 64)                0         \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 512)               33280     \n",
      "_________________________________________________________________\n",
      "flatten_2 (Flatten)          (None, 512)               0         \n",
      "_________________________________________________________________\n",
      "dense_5 (Dense)              (None, 512)               262656    \n",
      "_________________________________________________________________\n",
      "flatten_3 (Flatten)          (None, 512)               0         \n",
      "_________________________________________________________________\n",
      "dense_7 (Dense)              (None, 512)               262656    \n",
      "_________________________________________________________________\n",
      "flatten_4 (Flatten)          (None, 512)               0         \n",
      "_________________________________________________________________\n",
      "dense_9 (Dense)              (None, 512)               262656    \n",
      "_________________________________________________________________\n",
      "dense_10 (Dense)             (None, 10)                5130      \n",
      "=================================================================\n",
      "Total params: 849,674\n",
      "Trainable params: 849,674\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Error when checking target: expected dense_10 to have 2 dimensions, but got array with shape (14043, 10, 10)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-11-0125d58e90c2>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     17\u001b[0m               metrics=['accuracy'])\n\u001b[1;32m     18\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msummary\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 19\u001b[0;31m \u001b[0mtry_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-4-aa6d4cccb537>\u001b[0m in \u001b[0;36mtry_model\u001b[0;34m(epochs, save, batch_size, model)\u001b[0m\n\u001b[1;32m     35\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     36\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 37\u001b[0;31m     \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrainX\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mtrainy\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mepochs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     38\u001b[0m     \u001b[0mlossed\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0maccuracy\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mevaluate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtestX\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mtesty\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     39\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"====================================\\nModel \"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mtry_model\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcount\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\" : \"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/tensorflow/python/keras/_impl/keras/engine/training.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, **kwargs)\u001b[0m\n\u001b[1;32m   1141\u001b[0m         \u001b[0msample_weight\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msample_weight\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1142\u001b[0m         \u001b[0mclass_weight\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mclass_weight\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1143\u001b[0;31m         batch_size=batch_size)\n\u001b[0m\u001b[1;32m   1144\u001b[0m     \u001b[0;31m# Prepare validation data.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1145\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mvalidation_data\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/tensorflow/python/keras/_impl/keras/engine/training.py\u001b[0m in \u001b[0;36m_standardize_user_data\u001b[0;34m(self, x, y, sample_weight, class_weight, batch_size)\u001b[0m\n\u001b[1;32m    802\u001b[0m           \u001b[0mfeed_output_shapes\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    803\u001b[0m           \u001b[0mcheck_batch_axis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m  \u001b[0;31m# Don't enforce the batch size.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 804\u001b[0;31m           exception_prefix='target')\n\u001b[0m\u001b[1;32m    805\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    806\u001b[0m       \u001b[0;31m# Generate sample-wise weight values given the `sample_weight` and\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/tensorflow/python/keras/_impl/keras/engine/training_utils.py\u001b[0m in \u001b[0;36mstandardize_input_data\u001b[0;34m(data, names, shapes, check_batch_axis, exception_prefix)\u001b[0m\n\u001b[1;32m    181\u001b[0m                            \u001b[0;34m': expected '\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mnames\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m' to have '\u001b[0m \u001b[0;34m+\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    182\u001b[0m                            \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m' dimensions, but got array '\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 183\u001b[0;31m                            'with shape ' + str(data_shape))\n\u001b[0m\u001b[1;32m    184\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mcheck_batch_axis\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    185\u001b[0m           \u001b[0mdata_shape\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdata_shape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: Error when checking target: expected dense_10 to have 2 dimensions, but got array with shape (14043, 10, 10)"
     ]
    }
   ],
   "source": [
    "x = tf.keras.layers.Flatten()(x)\n",
    "\n",
    "# Create a fully connected layer with ReLU activation and 512 hidden units\n",
    "x = tf.keras.layers.Dense(512, activation='relu')(x)\n",
    "\n",
    "# Create output layer with a single node and sigmoid activation\n",
    "output = tf.keras.layers.Dense(10, activation='sigmoid')(x)\n",
    "\n",
    "# Create model:\n",
    "# input = input feature map\n",
    "# output = input feature map + stacked convolution/maxpooling layers + fully \n",
    "# connected layer + sigmoid output layer\n",
    "model = tf.keras.models.Model(img_input, output)\n",
    "\n",
    "model.compile(loss='categorical_crossentropy',\n",
    "              optimizer=tf.keras.optimizers.RMSprop(lr=0.001),\n",
    "              metrics=['accuracy'])\n",
    "model.summary()\n",
    "try_model(model=model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
